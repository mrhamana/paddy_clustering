{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4df76cbb",
   "metadata": {},
   "source": [
    "# Stratified Sampling, DBSCAN, and Isolation Forest Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8679ab4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32560dbb",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f547ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/paddydataset.csv')\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "410b84c2",
   "metadata": {},
   "source": [
    "## Stratified Sampling (From Scratch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e42f99c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stratified_sample(data, strata_column, sample_size=None, sample_fraction=0.2, random_seed=42):\n",
    "    np.random.seed(random_seed)\n",
    "    \n",
    "    strata = data[strata_column].unique()\n",
    "    sampled_data = []\n",
    "    \n",
    "    for stratum in strata:\n",
    "        stratum_data = data[data[strata_column] == stratum]\n",
    "        stratum_size = len(stratum_data)\n",
    "        \n",
    "        if sample_size:\n",
    "            n_samples = min(sample_size, stratum_size)\n",
    "        else:\n",
    "            n_samples = max(1, int(stratum_size * sample_fraction))\n",
    "        \n",
    "        indices = np.random.choice(stratum_data.index, size=n_samples, replace=False)\n",
    "        sampled_data.append(data.loc[indices])\n",
    "    \n",
    "    return pd.concat(sampled_data, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489e0f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "stratified_data = stratified_sample(df, strata_column='Variety', sample_fraction=0.3)\n",
    "print(f\"\\nOriginal dataset size: {len(df)}\")\n",
    "print(f\"Stratified sample size: {len(stratified_data)}\")\n",
    "print(f\"\\nOriginal variety distribution:\\n{df['Variety'].value_counts()}\")\n",
    "print(f\"\\nStratified sample variety distribution:\\n{stratified_data['Variety'].value_counts()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d812282",
   "metadata": {},
   "source": [
    "## Data Preprocessing for Clustering and Anomaly Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9daf44a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_features(data):\n",
    "    numeric_cols = data.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    features = data[numeric_cols].copy()\n",
    "    features = features.fillna(features.mean())\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    features_scaled = scaler.fit_transform(features)\n",
    "    \n",
    "    return features_scaled, numeric_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e73445",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_scaled, feature_names = prepare_features(df)\n",
    "print(f\"Number of features: {len(feature_names)}\")\n",
    "print(f\"Features shape: {features_scaled.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45db8af9",
   "metadata": {},
   "source": [
    "## DBSCAN Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c23866c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_dbscan(features, eps=0.5, min_samples=5):\n",
    "    dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "    clusters = dbscan.fit_predict(features)\n",
    "    \n",
    "    n_clusters = len(set(clusters)) - (1 if -1 in clusters else 0)\n",
    "    n_noise = list(clusters).count(-1)\n",
    "    \n",
    "    return clusters, n_clusters, n_noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8505980d",
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters, n_clusters, n_noise = apply_dbscan(features_scaled, eps=3.0, min_samples=10)\n",
    "\n",
    "df['cluster'] = clusters\n",
    "print(f\"Number of clusters: {n_clusters}\")\n",
    "print(f\"Number of noise points: {n_noise}\")\n",
    "print(f\"\\nCluster distribution:\\n{pd.Series(clusters).value_counts().sort_index()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cafb865e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_clusters(features, clusters, title=\"DBSCAN Clustering\"):\n",
    "    from sklearn.decomposition import PCA\n",
    "    \n",
    "    pca = PCA(n_components=2)\n",
    "    features_2d = pca.fit_transform(features)\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    scatter = plt.scatter(features_2d[:, 0], features_2d[:, 1], c=clusters, cmap='viridis', alpha=0.6)\n",
    "    plt.colorbar(scatter, label='Cluster')\n",
    "    plt.xlabel('PC1')\n",
    "    plt.ylabel('PC2')\n",
    "    plt.title(title)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "676e368e",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_clusters(features_scaled, clusters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "953b5918",
   "metadata": {},
   "source": [
    "## Isolation Forest Anomaly Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b9ceae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_anomalies(features, contamination=0.1, random_state=42):\n",
    "    iso_forest = IsolationForest(contamination=contamination, random_state=random_state)\n",
    "    predictions = iso_forest.fit_predict(features)\n",
    "    anomaly_scores = iso_forest.score_samples(features)\n",
    "    \n",
    "    return predictions, anomaly_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0da31bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions, anomaly_scores = detect_anomalies(features_scaled, contamination=0.05)\n",
    "\n",
    "df['anomaly'] = predictions\n",
    "df['anomaly_score'] = anomaly_scores\n",
    "\n",
    "n_anomalies = (predictions == -1).sum()\n",
    "n_normal = (predictions == 1).sum()\n",
    "\n",
    "print(f\"Normal points: {n_normal}\")\n",
    "print(f\"Anomalies detected: {n_anomalies}\")\n",
    "print(f\"Percentage of anomalies: {n_anomalies/len(df)*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "782e5229",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_anomalies(features, predictions, scores, title=\"Isolation Forest Anomalies\"):\n",
    "    from sklearn.decomposition import PCA\n",
    "    \n",
    "    pca = PCA(n_components=2)\n",
    "    features_2d = pca.fit_transform(features)\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    colors = ['red' if pred == -1 else 'blue' for pred in predictions]\n",
    "    axes[0].scatter(features_2d[:, 0], features_2d[:, 1], c=colors, alpha=0.6)\n",
    "    axes[0].set_xlabel('PC1')\n",
    "    axes[0].set_ylabel('PC2')\n",
    "    axes[0].set_title(f'{title} - Classification')\n",
    "    axes[0].legend(['Normal', 'Anomaly'])\n",
    "    \n",
    "    scatter = axes[1].scatter(features_2d[:, 0], features_2d[:, 1], c=scores, cmap='coolwarm', alpha=0.6)\n",
    "    axes[1].set_xlabel('PC1')\n",
    "    axes[1].set_ylabel('PC2')\n",
    "    axes[1].set_title(f'{title} - Anomaly Scores')\n",
    "    plt.colorbar(scatter, ax=axes[1], label='Score')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "302a953a",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_anomalies(features_scaled, predictions, anomaly_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e704e28",
   "metadata": {},
   "source": [
    "## Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b9fc86c",
   "metadata": {},
   "outputs": [],
   "source": [
    "anomalies_df = df[df['anomaly'] == -1].copy()\n",
    "normal_df = df[df['anomaly'] == 1].copy()\n",
    "\n",
    "print(\"Top 10 anomalies by score:\")\n",
    "print(anomalies_df.nsmallest(10, 'anomaly_score')[['Variety', 'Paddy yield(in Kg)', 'anomaly_score', 'cluster']])"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
